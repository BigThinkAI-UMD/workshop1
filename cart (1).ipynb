{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing warnings module to suppress warnings\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing necessary libraries\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    1/10000 [..............................] - ETA: 58:32 - reward: 1.0000done, took 1.651 seconds\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 30.000, steps: 30\n",
      "Episode 2: reward: 33.000, steps: 33\n",
      "Episode 3: reward: 119.000, steps: 119\n",
      "Episode 4: reward: 96.000, steps: 96\n",
      "Episode 5: reward: 35.000, steps: 35\n",
      "Episode 6: reward: 116.000, steps: 116\n",
      "Episode 7: reward: 36.000, steps: 36\n",
      "Episode 8: reward: 60.000, steps: 60\n",
      "Episode 9: reward: 34.000, steps: 34\n",
      "Episode 10: reward: 45.000, steps: 45\n",
      "60.4\n"
     ]
    }
   ],
   "source": [
    "# Creates an environment for the CartPole-v1 task\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Retrieves the number of state variables from the environment observation space\n",
    "states = env.observation_space.shape[0]\n",
    "\n",
    "# Retrieves the number of actions available in the environment\n",
    "# (In the case of CartPole, there are two possible actions: move left or move right)\n",
    "actions = env.action_space.n\n",
    "\n",
    "# Defines the underlying neural network model using TensorFlow's Keras Sequential API\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, states)))\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dense(actions, activation=\"linear\"))\n",
    "\n",
    "# Initializes a DQN agent using the above-defined model\n",
    "agent = DQNAgent(\n",
    "    model=model,\n",
    "    memory=SequentialMemory(limit=50000, window_length=1),\n",
    "    policy=BoltzmannQPolicy(),\n",
    "    nb_actions=actions,\n",
    "    nb_steps_warmup=10,\n",
    "    target_model_update=0.01\n",
    ")\n",
    "\n",
    "# Compiles the DQN agent using the Adam optimizer and MAE metric\n",
    "agent.compile(Adam(learning_rate=0.1), metrics=[\"mae\"])\n",
    "\n",
    "# Trains the DQN agent on the environment\n",
    "agent.fit(env, nb_steps=50000, visualize=False, verbose=1)\n",
    "\n",
    "# Tests the trained agent on the environment for evaluation\n",
    "res = agent.test(env, nb_episodes=10, visualize=True)\n",
    "\n",
    "# Prints the average reward obtained by the agent during testing\n",
    "print(np.mean(res.history[\"episode_reward\"]))\n",
    "\n",
    "# Closes the environment after training and testing\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
